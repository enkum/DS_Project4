---
title: '**\center{MODELING FOOD AND HOUSING}** \center{\textbf{INSECURITY}} \center{by}'
author: "Francis Biney, Ebenezer Nkum and Tolulope Adeyina"
output:
  pdf_document:
    keep_tex: yes
    fig_caption: yes
    latex_engine: pdflatex
    number_sections: yes
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '4'
    df_print: paged
geometry: margin = 0.98in
fontsize: 12pt
header-includes:
- \usepackage {setspace}\onehalfspacing
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsfonts}
- \usepackage{amsthm}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhf{}
- \lhead{DS 6335}
- \rhead{FHI}
- \cfoot{\thepage}
- \usepackage{algorithm}
- \usepackage[noend]{algpseudocode}
- \usepackage[none]{hyphenat}
- \usepackage{xcolor}
- \usepackage{sectsty}
- \usepackage{graphicx}
- \chapterfont{\color{blue}}  % sets colour of chapters
- \sectionfont{\color{black}}  % sets colour of sections
- \subsectionfont{\color{black}}  % sets colour of subsections
- \subsubsectionfont{\color{black}}  % sets colour of subsubsections
bibliography: ["ref.bib"]
biblio-style: "apalike"
link-citations: true
---

\noindent\rule{17.5cm}{0.8pt}

\newpage


```{r, echo= FALSE}
setwd("C://Users//enkum//OneDrive - University of Texas at El Paso//Desktop//Intro_Data_Collaboratiosn//Project-first-phase")
```



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include= FALSE}

# LIBRARIES----
library(caret)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(skimr)
library(rsample)
library(vip)
library(funModeling)
library(Hmisc)
library(lattice)

```

# Executive Summary

This project set out to find which subpopulations of University of Texas at El Paso (UTEP) students
are most at risk for food and housing insecurity. The data set used is a direct survey constructed to assess thet state of food and housing insecurity among UTEP students by a team of researchers (Moya, Crouse, Schober and Wagler). Each student participants record consists of $39$ variables, containing sociodemographic data (variables 1-19) and data about food and housing status (variable 20-37). There were $5,175$ total participants, used for this project. Among the variables collected, variables; **In the past 12 months, were you ever hungry but didn't eat because there wasn't enough money for food?** and **In the last 12 months, since (today's date),did you ever cut the size of your meals or skip meals because there was not enough money for food?** were used to assess those at risk for food insecurity, and variables; 
**In the past 12 months, have you had a permanent address?** and **Due to lack of permanent address or housing options, how frequently did you spend the night elsewhere in the past six months due to lack of permanent housing?** were used to assess those at risk for housing insecurity. We would like to mention that, the variables that were used to assess the housing insecurity the former suffered from imbalance proportion of representation and the latter suffered from lack of representation. However models used have some tendency to correct these imbalances to a certain degree.


# Main Report
Our analysis reveals that debt increase is the strongest single predictor of food insecurity among UTEP students. The other predictors that are most statistically significant are:
\begin{itemize}
\item Age
\item Total income (less $10,000$)
\item Federal Aid in the past 12 month (about the same)
\item Income change (about the same)
\item Any benefit support (Supplemental Nutrition Assistance Program)
\item College (Liberal Arts)
\item Hours work per week ($19$ hrs or less)
\item Expenditure change (about the same)
\end{itemize}
These factors have  $76.91\%$ accuracy of identifying students who are at risk for food insecurity with sensitivity of $27.5\%$ and specificity of $93.33\%$. Intuitively, these factors classify subpopulation of students who have their debt increased, with total annual income below $10,000$, having their expenditure decreased and head of household is $30\%$ at risk of food insecurity.\newline
Also, the analysis reveals that students staying on-campus or off-campus not with family is the strongest predictor of housing insecurity among UTEP students.  The other predictors that are most statistically significant are:
\begin{itemize}
\item Age 
\item Mode of transport (Bike and walk)
\item Total Income (below $10,000$ anually)
\item Income change (About the same)
\item College (of Education)
\item Hours work per week ($19$ hrs or less)
\item federal Aid in the past six month (Work study)
\end{itemize}

These factors have  $95.44\%$ accuracy of identifying students who are at risk for housing insecurity with sensitivity of $3.3\%$ and specificity of $99.99\%$. Intuitively, these factors classify subpopulation of students who have stay on-campus or off-campus not with family with age less $27$, with mode of transport (carpool, Bus, Trolley, Bike, Walk) and federal aid about the same to at risk for housing insecurity.
\newline
These finding is actionable and it was not obvious in advance. A student with increased debt presumably will be at risk for food  insecurity.

# Recommendation

\begin{itemize}
\item The major challenge is correct the data representation. Correct wording of questions to get the right response should be considered.
\item We recommend that this subpopulation identified here should be closely studied to validate the findings 
\end{itemize}

# Detail Analysis

## Introduction
In this project, we seek to find factors that are associated with food insecurity and housing insecurity among University of Texas at El Paso (UTEP) students. We further examined subpopulations of students who are at most risk for food and housing Insecurity. We employed the well-know models of logistic regression, logistic regression with LASSO regularization, random forest and classification tree and choose the best model with highest accuracy. In recent years, there has been an increasing awareness among educators about the prevalence of food and housing insecurities in college and university settings. Largely, this is due to the efforts of scholars such as [@gupton2014engaging] and [@goldrick2015hungry] (among others), who have succeeded in raising awareness about extreme cases of insecurities, such as hunger and homelessness. As the United States simultaneously endures a historic pandemic and an economic recession, many college students are having trouble accessing basic needs. A recent survey from the Hope Center for College, Community, and Justice,[@goldrick2020realcollege] found that more than half of students are experiencing food insecurity, housing insecurity, or homelessness. In addition, more than two-thirds of students lost a job or suffered cuts to pay or hours, and many have been unable to get financial assistance from their campus or the federal government. One of the survey’s most troubling findings is that students of color—especially Black students, Pacific Islander or Native Hawaiian students, and Indigenous students—are being disproportionately affected. We seek to find what may be specific among UTEP students.

## Data
The data come from an electronic survey completed by $5449$ students attending UTEP at all levels. There are 5449 rows each representing responses of the participants. There are $37$ distinct variables (Questions) for the survey. There are $36$ categorcal variables and only one numerical variable (Age). Out of the $37$ distinct variables, there two variables that can be used as a response variable for housing insecurity model and four variables that can be used a response variable for the food insecurity model. The main challenge in processing this data set was the large number of missing values in the response variables.

## Data Preprocessing and Feature Engineering

The original data contain multiple columns for some specific questions. We collapse them into one column with their respective coding. The original data contain $7,087$ respondents who received an ID but responded to no question. We deleted all of them. We also deleted all respondents who dropped after answering their 5th question. We removed irrelevant variables with no predictive power from the data. We corrected inconsistency values in the **Age** variable. The final data before the impute had $5,175$ rows and $30$ columns.
\newline
**Missing Values Treatment**
\begin{itemize}
\item We imputed two variables with missing rate less $50\%$ with modal class.
\item We imputed the rest of missing values with random forest algorithm.
\item Even though one response variable for the housing insecurity model has missing rate of 95.09\%, we decided to fit a sub-model with the data present (Down sampling).
\end{itemize}


## Data Exploration
With data distribution, approximately 84\% of those who participated in the survey were full students, with 16\% being part time students. 42.53\% of the participants are working and the 57.47\% are not working. Hispanics/Latins make about 76\% of the entire respondents.  






# Analysis plan

## Model Development
We fitted logistic regression (both multinomial and binomial), logistic regression with LASSO regularization and random forest to assess the  risk factors housing insecurity. We obtained the variable important and determine the highest predictor of the response variable.
\newline
The logistic regression model is widely used in the social and biological sciences. The model is especially useful is demographic research in the assessment of the effects of the explanatory factors on the relative risk of outcomes. In this case, the logistic model will provide the probability of a particular outcome occurring. It supports categorizing data into discrete classes by studying the relationship from a given set of labeled data. It is easier to implement and makes no assumption about the distributions of the classes un feature space. 
\newline
The second model we explore is the LASSO regularization of the logistic regression. We need regularization to introduce bias to the model and to decrease the variance. This method will set regression coefficients for irrelevant variables to zero. This provides a system for selecting important variables but it does not necessarily provide a way to rank them. 
\newline
Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way.[@breiman2001random]

In the project we have 3 questions to answer:

\begin{itemize}
\item Which factors are associated with housing Insecurity ?
\item Which factors are associated with food insecurity?
\item Which subpopulations are most at risk for Food and housing Insecurity?
\end{itemize}

**Housing Insecurity**
With the housing insecurity, we have two response variable to help us identify the category of the students sample that are at the risk of housing insecurity. The responses we explored were:
\begin{itemize}
\item Due to lack of permanent address or housing options, how frequently did you spend the night elsewhere in the past six months due to lack of permanent housing?
\item In the past 12 months, have you had a permanent address?
\end{itemize}

The first response variable has three (3) categorical variable (often, sometimes and rarely) and the second response has 2 categorical variable (Yes, No). However we had 95.09\% missing rate so we decided to fit a sub-model with the data present. \newline
**Food Insecurity**
\newline
Here we have 4 possible response variables to explore. We examined 2 of them to determine which factors are associated with food insecurity.
\newline
The responses we explored were:
\begin{itemize}
\item Q28. In the last 12 months, since (today's date), did you ever cut the size of  your meals or skip meals because there was not enough money for food?
\item Q31. In the past 12 months, were you ever hungry but didn't eat because there wasn't enough money for food? 
\end{itemize}

Each of these have two categorical variable. We report the best model with highest accuracy. We also check the consistency of the variable importance with the different response. We performed some exploration on the first 3 variable of importance and provided recommendation. 



# Analysis and Results
## Housing Insecurity model
```{r, include= FALSE}
# remove
#experience_of_homelessness,food_not_last,skip_meals, eat_less, go_hungry

df_new <- read.csv("House_and_food.csv", header=T, na.strings = c("", "NA"))

df_imputed_housing <- df_new[ ,-c(1,20,23,24,25,26)]


df_new <-  df_new %>% mutate_if(is.character,as.factor)
df_new <-   df_new%>% mutate_if(is.integer,as.factor)
df_new$Age<- as.double(df_new$Age)

df_new$Hours_work_per_week <- factor(df_new$Hours_work_per_week, ordered = T)
df_new$total_income        <- factor(df_new$total_income, ordered = T)
df_new$academic_level      <- factor(df_new$academic_level, ordered = T)

df_imputed_housing <- df_new[ ,-c(1,20,23,24,25,26)]
```



```{r, echo = FALSE}
# Race housing distribution

c = as.data.frame(table(df_imputed_housing$permanent_address_past12,
                                             df_imputed_housing$Race))

p <- ggplot(data = c, aes(x=Var2, y= Freq, fill=Var1)) +
     geom_bar(stat="identity", position=position_dodge())


print(p  + labs(title= "Race housing distribution",
                      x="Race", fill = "Permanent address"))

```


```{r, echo= FALSE}
# academic housing distribution

c = as.data.frame(table(df_imputed_housing$permanent_address_past12,
                                              df_imputed_housing$academic_level))

p <- ggplot(data = c, aes(x=Var2, y= Freq, fill=Var1)) +
     geom_bar(stat="identity", position=position_dodge())


print(p  + labs(title= "Academic level housing distribution",
                      x="Academic level", fill = "Permanent address"))

```




## Housing Insecurity mode (l) : First response variable ( Permanent address past 12 month)

```{r , include = FALSE}
df_housing1 <- df_imputed_housing

# Remove similar response
df_housing1<- df_housing1 %>% 
                      mutate(permanent_address_past12=
                      recode(permanent_address_past12
                      ,`1` = "1" 
                      ,`2` ="0")) 


df_housing1 <- df_housing1 %>% rename(house_ins1=permanent_address_past12) 

#* Train test spit------------

set.seed(123)
df_split1 <- initial_split(df_housing1, strata = house_ins1) # 3/4 train 1/4 test
df_train_h <- training(df_split1)
df_test_h <- testing(df_split1)

```



```{r, include = FALSE}

#library(caret)

myControl = trainControl(method = "cv", number = 10)

#* Model  1----
set.seed(123)

multiLog_h = train(
  house_ins1 ~ .,
  data = df_housing1,
  method = "glm",
  family = binomial(link = "logit"),
  trControl = myControl,
  trace = FALSE
)


```



```{r, echo= FALSE}
vip(multiLog_h )

multiLog_h 

#Test Prediction----

pred <- predict(multiLog_h, df_test_h, type="raw" ) # type: class=class, response=class prob
result <- confusionMatrix(pred, df_test_h$house_ins1, positive = "1",mode =  "everything")
result

```

Binomial logistic regression gave the highest accuracy. We use 10-fold cross validation to estimate the model parameters. The results are reported below: We report the confusion matrix and variable of importance for the model. At an accuracy of $95.44\%$; Age, place of stay, mode of transport and total income were found to be the first 4 important variables.


## Housing Insecurity model : Second response variable ( Spent the night elsewhere)

## Housing Insecurity model (2)

```{r, include= FALSE}
df_new2 <- read.csv("final_data_housing_model_1.csv", header=T, na.strings = c("", "NA"))


df_new2 <-  df_new2 %>% mutate_if(is.character,as.factor)
df_new2 <-   df_new2%>% mutate_if(is.integer,as.factor)
df_new2$Age<- as.double(df_new2$Age)

df_new2$Hours_work_per_week <- factor(df_new2$Hours_work_per_week, ordered = T)
df_new2$total_income        <- factor(df_new2$total_income, ordered = T)
df_new2$academic_level      <- factor(df_new2$academic_level, ordered = T)

df_only_254_cases1 <- df_new2[ ,-c(1,19,20,23,24,25,26)]
```


```{r, include= FALSE}
## Housing Insecurity model

df_house2 <- df_only_254_cases1


df_house2<-df_house2 %>% rename(night=night_elsewhere) 

#* Train test spit------------
set.seed(123)
df_split <- initial_split(df_house2, strata = night) # 3/4 train 1/4 test
df_train <- training(df_split)
df_test <- testing(df_split)

```



```{r, echo = FALSE}

# RandomForest(Model 3)

#model with default paramters -----
set.seed(123)
tunegrid <- expand.grid(.mtry=c(1:15))
rf_1 <- train(night~., 
                    data=df_train, 
                    method="rf", 
                    metric="Accuracy", 
                    tuneGrid= tunegrid, 
                    trControl=trainControl(method="repeatedcv", number=10, 
                                           repeats=3))


#RandomForest(Model 3)
print(rf_1)


# VIP------
vip(rf_1)

#* Test Prediction----
pred <- predict(rf_1, df_test, type="raw" ) # type: class=class, response=class prob
result <- confusionMatrix(pred, df_test$night,positive = "3",mode =  "everything")
result

```
Random forest gave the highest accuracy for prediction. At an accuracy of $55.38\%$; Age, income change, benefit support and debt change were found to be first 4 variable of importance.



# Food Insecurity model


## Using the response variable : In the past 12 months, were you ever hungry but didn't eat because
there wasn't enough money for food? 

```{r, include= FALSE}
df_food1 <- read.csv("food.csv", header=T, na.strings = c("", "NA"))

df_food1 <- df_food1 %>% mutate_if(is.character,as.factor)
df_food1 <-   df_food1%>% mutate_if(is.integer,as.factor)
df_food1$Age<- as.double(df_food1$Age)

df_food1$Hours_work_per_week <- factor(df_food1$Hours_work_per_week, ordered = T)
df_food1$total_income        <- factor(df_food1$total_income, ordered = T)
df_food1$academic_level      <- factor(df_food1$academic_level, ordered = T)

df_food_new_1 <- df_food1[ ,-c(1,19,20,21,27)]

```



```{r}
# Academic level food insecurity distribution



c = as.data.frame(table(df_food_new_1$go_hungry,df_food_new_1$academic_level))

p <- ggplot(data = c, aes(x=Var2, y= Freq, fill=Var1)) +
     geom_bar(stat="identity", position=position_dodge())


print(p  + labs(title= "Academic level food insecurity distribution",
                      x="Academic", fill = "Go hungry"))

```



```{r}
# Racial housing distribution

c = as.data.frame(table(df_food_new_1$go_hungry,df_food_new_1$Race))

p <- ggplot(data = c, aes(x=Var2, y= Freq, fill=Var1)) +
     geom_bar(stat="identity", position=position_dodge())


print(p  + labs(title= "Racial food insecurity distribution",
                      x="Race", fill = "Go hungry"))

```




```{r, include= FALSE}
df_food2 <- df_food_new_1

# Remove similar response
df_food2<- df_food2 %>% mutate(go_hungry=recode(go_hungry
                      ,`1` = "1" 
                      ,`2` ="0")) 


df_food2 <- df_food2 %>% rename(food_ins3=go_hungry) 

#* Train test spit------------

set.seed(123)
df_split3 <- initial_split(df_food2, strata = food_ins3) # 3/4 train 1/4 test
df_train3 <- training(df_split3)
df_test3 <- testing(df_split3)

```



```{r, include= FALSE}

# Food Insecurity Model: Second Response variable

# RandomForest  (Model 3)

doParallel::registerDoParallel()
set.seed(123)

myControl = trainControl(method="cv", number=10)

rf_f <- train(food_ins3~., 
              data=df_train3, 
              method="rf", 
              metric="Accuracy", 
              tunegrid = expand.grid(.mtry=3),
              trControl= myControl)



print(rf_f)

```

```{r, echo= FALSE}

#VIP------
vip(rf_f)

```


```{r, echo= FALSE}

## --Test Prediction----
pred <- predict(rf_f, df_test3, type="raw" ) # type: class=class, response=class prob
result <- confusionMatrix(pred, df_test3$food_ins3,positive = "1",mode = "everything")
result
```
Random forest gave the highest accuracy for prediction. At an accuracy of $76.91\%$; Age, debt change, total income and benefit support were found to be first 4 variable of importance.



## Using the response variable : eat_less

```{r, include=FALSE}
df_food_sec <- df_food1

# Remove similar response
df_food_sec<- df_food_sec %>% 
  select(-X,-food_not_last,-skip_meals,
         -go_hungry,-night_elsewhere) %>% mutate(eat_less=recode(eat_less
                      ,`1` = "1" 
                      ,`2` ="0")) 


df_food_sec <- df_food_sec %>% rename(food_ins1=eat_less) 

#* Train test spit------------

set.seed(123)
df_split1 <- initial_split(df_food_sec, strata = food_ins1) # 3/4 train 1/4 test
df_train1 <- training(df_split1)
df_test1 <- testing(df_split1)

```


```{r, include= FALSE}

myControl = trainControl(method = "cv", number = 10)

#* Model  1----
set.seed(123)

multiLog1 = train(
  food_ins1 ~ .,
  data = df_train1,
  method = "glm",
  family = binomial(link = "logit"),
  trControl = myControl,
  trace = FALSE
)


```


```{r, echo= FALSE}
vip(multiLog1)

multiLog1

#* Test Prediction----
pred <- predict(multiLog1, df_test1, type="raw" ) # type: class=class, response=class prob
result <- confusionMatrix(pred, df_test1$food_ins1, positive = "1",mode =  "everything")
result

```
Binomial logistic regression gave the highest accuracy. We use 10-fold cross validation to estimate the model parameters. The results are reported below: We report the confusion matrix and variable of importance for the model. At an accuracy of $71.51\%$; total income , income change, debt change , and expenditure change were found to be the first 4 important variables.



# Subpopulation at risk for food and housing insecurity.

## Food Insecurity
The most important predictors with response, go hungry because there was no money for by the highest performing model with accuracy of $76.91\%$ are 

\begin{itemize}
\item Age
\item Debt Change
\item Total income
\item Federal Aid in the past 12 month
\item Income change
\item Any benefit support
\item College
\item Hours work per week
\item Expenditure change
\end{itemize}

```{r}
df_food_tree <- df_food2      #df_food2[, c(5,22,9,20,17,16,11,4,19,18)]
```


```{r ,include= FALSE}
# Load data
#library(tidymodels)
library(ISLR)
library(rpart.plot)

df_fd_2 <- df_food_tree

df_fd_2 <-df_fd_2 %>% mutate(food_ins3=recode(food_ins3
                      ,`1` = "Yes" 
                      ,`0` ="No")) 



# build a large tree
set.seed(2)
treefit = rpart(food_ins3~., method="class"
                #,parms=list(split='gini')
                ,control=rpart.control(cp=0)
                ,data=df_fd_2)

cv.tree = printcp(treefit)

plotcp(treefit,col=2)


mincv = which.min(cv.tree[,4])
cutoff = cv.tree[mincv,4] + cv.tree[mincv,5]
whichcp = min(which(cv.tree[,4] < cutoff))
mycp = cv.tree[whichcp,1]
mycp

 
 
 # Refit tree with optimal complecity
treefit = rpart(food_ins3~., method="class"
                #,parms=list(split='gini')
                ,control=rpart.control(cp=mycp ,minsplit = 50)
                ,data=df_fd_2)



```


```{r, echo = FALSE}
#plot tree
cols = ifelse(treefit$frame$yval == 2, "darkorange", "darkgreen")
rpart.plot(treefit, under=TRUE, faclen=0, extra=106, under.col="black", col=cols)


```



## Housing Insecurity

The most important predictors with response variable, **permanent address** by the correct predicting model with accuracy of $95.44\%$ are:

\begin{itemize}
\item Age
\item Place of stay
\item mode of transportation
\item Total income
\item Income change
\item College
\item Hours work per week
\item federal Aid past 12 month
\end{itemize}


```{r, include= FALSE}
df_house_tree <- df_housing1
```




```{r ,include= FALSE}
# Load data
set.seed(2)

df_fd_3 <- df_house_tree

df_fd_3 <-df_fd_3 %>% mutate(house_ins1=recode(house_ins1
                      ,`1` = "Yes" 
                      ,`0` ="No")) 



# build a large tree
set.seed(2)
treefit1 = rpart(house_ins1~., method="class"
                #,parms=list(split='gini')
                ,control=rpart.control(cp=0)
                ,data=df_fd_3)

cv.tree1 = printcp(treefit1)

plotcp(treefit1,col=2)


mincv = which.min(cv.tree1[,4])
cutoff = cv.tree1[mincv,4] + cv.tree1[mincv,5]
whichcp = min(which(cv.tree1[,4] < cutoff))
mycp1 = cv.tree1[whichcp,1]
mycp1

 
 
 # Refit tree with optimal complecity
treefit1 = rpart(house_ins1~., method="class"
                #,parms=list(split='gini')
                ,control=rpart.control(cp=0 ,minsplit = 50)
                ,data=df_fd_3)

```


```{r, echo= FALSE}
#plot tree
cols = ifelse(treefit1$frame$yval == 2, "darkorange", "darkgreen")
rpart.plot(treefit1, under=TRUE, faclen=0, extra=106, under.col="black", col=cols)


```




# Discussion


## Housing Insecurity model

Three models were built for the housing insecurity response: Q22:"frequency of night elsewhere in the past six months due to lack of permanent". Q22 has three categories: rarely, sometimes, and often.\newline
The models consider are Multinomial logistic regression(MLR), Multinomial logistic regression with LASSO penalty (MLR-LASSO) and Random Forest (RF). 10-fold cross validation was used to select the optimal hyper-parameter. Random forest gave the best prediction  accuracy of 0.585 with confidence interval(CI) of (0.456, 0.706)  followed by MLR-LASSO with prediction accuracy of $55.38\%$ with $95\%$ CI : ($0.4253, 0.6773$). The top five risk factor for each response from RF's variable important plots are Age(Q5), any_benefit_support(Q24),income_change(Q33), depth_change(Q35) and experience_homelessness(Q13)


## Food Insecurity model

We built three models for three  food insecurity response and determined the top 5 feature or  risk factors that  explains these response. The response considered are  Q28: "In the last 12 months, since (today's date), did you ever cut the size of  your meals or skip meals because there was not enough money for food?" and Q31:"Were you ever hungry but didn't eat because the wasn't enough money for food". \newline
The models consider in each case are Multinomial logistic regression(MLR), binomial logistic regression with LASSO penalty (MLR-LASSO) and Random Forest (RF). \newline
For the first response (Q26) the model that gave the best prediction was binomial logistic regression with prediction accuracy of $71.51\%$ and  $95\%$ confidence interval of ($0.6896, 0.7395$). The best model for Q31 was Random Forest with prediction accuracy of $76.91\%$ and confidence interval of ($0.7452, 0.7918$). 
The top five risk factor for each response form the estimated coefficients and variable important plots for response Q26 are Total_income (Q9), Place_of_stay (Q19), Race (Q6),Reliability_of_transportation (Q13) and experience_homelessness(Q23). For Q28 the risk factor are depth_change(Q35), experience homelessness(Q23), income_change(Q33) and reliability_of_transportation(Q13). Risk factors for Q31 are Total income (Q9), experience_homelessness(Q13), Race (Q6), depth_change(Q35) and income_change(Q33).

# References


